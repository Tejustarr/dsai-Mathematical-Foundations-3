{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Numerical differentiation and gradient checking \u2014 experiments with step size and error analysis"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\nf = lambda x: np.sin(x) * x**2\ndef central_diff(f,x,h):\n    return (f(x+h)-f(x-h)) / (2*h)\n\nxs = [1e-1,1e-2,1e-3,1e-4,1e-5]\nfor h in xs:\n    approx = central_diff(f, 0.37, h)\n    print(h, approx)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Gradient checker for multivariate functions (central differences)"]}, {"cell_type": "code", "metadata": {}, "source": ["def grad_check(f, x, analytic_grad, eps=1e-6):\n    # f: R^n -> R\n    x = np.array(x, dtype=float)\n    n = len(x)\n    num_grad = np.zeros(n)\n    for i in range(n):\n        dx = np.zeros(n); dx[i]=eps\n        num_grad[i] = (f(*(x+dx)) - f(*(x-dx)))/(2*eps)\n    return np.linalg.norm(num_grad - analytic_grad)\n\n# test\nf2 = lambda a,b: a**2 * np.sin(b)\nanalytic = np.array([2*0.7*np.sin(0.3), 0.7**2*np.cos(0.3)])\nprint('grad check error=', grad_check(lambda a,b: f2(a,b), [0.7,0.3], analytic))"], "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 5}