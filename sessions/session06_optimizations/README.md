# Session 6 — Function Optimizations (Deep)

**Duration:** 180 minutes  
**Prerequisites:** Sessions 1–5 (linear algebra, derivatives, gradients).  

**Topics:**
- Introduction to optimization: maxima & minima
- Convex vs non-convex functions
- First-order optimization algorithms
- Gradient Descent (GD)
- Stochastic Gradient Descent (SGD) and variants
- Learning rate schedules and convergence

**Learning Outcomes:**
- Identify and classify critical points (local/global, convexity)
- Understand first-order optimality conditions
- Implement gradient descent for convex & non-convex functions
- Implement stochastic gradient descent for ML problems
- Compare optimization algorithms experimentally
