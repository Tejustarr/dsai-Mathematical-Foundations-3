# Session 6 — Optimization Methods

**Session learning outcomes**:
- Understand first-order optimization: GD, SGD, momentum, adaptive methods
- Implement gradient descent variants and observe behavior
- Relate step-size, conditioning, and convergence

## Notebooks (with short descriptions)
- `01_optimization_basics.ipynb` — Maxima/minima, convex vs non-convex intuition.
- `02_gradient_descent.ipynb` — Batch GD derivation and code, learning rate effects.
- `03_sgd_and_variants.ipynb` — SGD, mini-batch, momentum, Adam (math + pseudocode).

## Exercises
- `optimization_quiz.md` — Problems on convergence, step-size selection, and proofs for convex quadratic.
- `experiments_lab.ipynb` — Compare GD/SGD/Adam on simple losses; visualize trajectories.

## Mini-project
- `logistic_regression_optimization.ipynb` — Train logistic regression via different optimizers; tune hyperparameters; report results.
