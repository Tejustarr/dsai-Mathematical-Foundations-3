{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Mini-Project Starter: Logistic Regression (Derive & Implement)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Load data and preprocess"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\nimport pandas as pd\nfrom pathlib import Path\np = Path('../04_datasets/logistic_data.csv')\ntry:\n    df = pd.read_csv(p)\nexcept Exception:\n    # fallback synthetic\n    df = pd.DataFrame({'x1':[0.5,1.0,1.5,2.0,2.5],'x2':[1.0,1.2,0.8,1.5,2.2],'y':[0,0,1,1,1]})\nX = df[['x1','x2']].values\ny = df['y'].values\nX = np.hstack([np.ones((X.shape[0],1)), X])  # add intercept\nprint('X shape', X.shape)\nprint(df.head())"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Define sigmoid and loss"]}, {"cell_type": "code", "metadata": {}, "source": ["def sigmoid(z):\n    return 1/(1+np.exp(-z))\n\ndef neg_log_likelihood(w,X,y):\n    z = X.dot(w)\n    p = sigmoid(z)\n    # small epsilon for numerical stability\n    eps = 1e-12\n    return -np.sum(y*np.log(p+eps) + (1-y)*np.log(1-p+eps))\n\nw = np.zeros(X.shape[1])\nprint('initial loss', neg_log_likelihood(w,X,y))"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Analytic gradient and Hessian (to implement)"]}, {"cell_type": "code", "metadata": {}, "source": ["def grad_and_hessian(w,X,y):\n    z = X.dot(w)\n    p = sigmoid(z)\n    grad = X.T.dot(p - y)\n    # Hessian: X^T D X where D is diag(p*(1-p))\n    D = np.diag(p*(1-p))\n    H = X.T.dot(D).dot(X)\n    return grad, H\n\nprint('grad,hess shapes', grad_and_hessian(w,X,y)[0].shape, grad_and_hessian(w,X,y)[1].shape)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Implement Newton's method & Gradient Descent templates (fill in)"]}, {"cell_type": "code", "metadata": {}, "source": ["def gradient_descent(w0, X, y, lr=0.1, iters=1000):\n    w = w0.copy()\n    losses = []\n    for i in range(iters):\n        g,_ = grad_and_hessian(w,X,y)\n        w = w - lr * g\n        losses.append(neg_log_likelihood(w,X,y))\n    return w, losses\n\ndef newtons_method(w0, X, y, iters=20):\n    w = w0.copy()\n    losses = []\n    for i in range(iters):\n        g,H = grad_and_hessian(w,X,y)\n        # solve H p = g\n        try:\n            p = np.linalg.solve(H, g)\n        except np.linalg.LinAlgError:\n            p = np.linalg.pinv(H).dot(g)\n        w = w - p\n        losses.append(neg_log_likelihood(w,X,y))\n    return w, losses\n\nw0 = np.zeros(X.shape[1])\nw_gd, losses_gd = gradient_descent(w0,X,y,lr=0.1,iters=200)\nw_nm, losses_nm = newtons_method(w0,X,y,iters=20)\nprint('final gd loss', losses_gd[-1])\nprint('final newton loss', losses_nm[-1])"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Plot convergence (students fill in)"]}, {"cell_type": "code", "metadata": {}, "source": ["import matplotlib.pyplot as plt\nplt.plot(losses_gd,label='GD')\nplt.plot(losses_nm,label='Newton')\nplt.yscale('log')\nplt.legend(); plt.xlabel('iter'); plt.ylabel('loss'); plt.show()"], "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 5}